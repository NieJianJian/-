## 慕课网——移动音视频入门

### 第一章 万人直播架构和CDN网络

#### 1. 直播产品的分类

* 泛娱乐化直播

  花椒、映客、斗鱼和虎牙等直播。

* 实时互动直播

  音视频会议、教育直播等，像思科、声网等。

#### 2. 泛娱乐化直播架构

通常分为四部分：

* 共享端（主播端）
* 信令服务器（处理信令，比如创建房间，聊天等都是信令的形式发送）
* CDN：流媒体云
* 观看端（普通用户）

主播是如何分享，观众是如何观看的？主要是以下几步：

1. 共享端发送创建房间信令到信令服务器，表示我要创建房间；
2. 服务端收到信令，在服务端创建好房间，之后返回给共享端一个流媒体云地址；
3. 共享端采集自己的音视频数据，形成rtmp流，推送到CDN网络；
4. 观众想要观看主播的节目，会发送信令到到信令服务器，服务器返回给观看端一个流媒体云地址；
5. 观看端根据流媒体云的地址，向CDN网络拉取相应的流，进行观看。

#### 3. 实时互动直播架构

TCP协议是可靠的流传输协议，必须有序，保证收到：发送、确认、超时、重发。

TCP可以等待3次超时，每次等待时间都是成倍增加。网络不好，就会拥塞控制，延迟发送。

TCP无法达到实时传输。

UDP传输，自己发送自己的，不管你能不能收到。可以用来进行实时互动直播。

实时互动直播架构如下：

1. 自有网络：UDP没有现成的网络，需要自己创建。

2. 服务多节点：一旦一个节点出现问题，会把这个节点的所有用户切换到另一个节点，对用户是无感知的。还有就是保证每个节点负载均衡，防止出现问题。
3. 控制中心：既然有多个节点，就需要有控制中心来管理；节点和控制中心之间通过心跳包来实时保持通讯，节点定时向控制中心报告自己的CPU、网络等情况；控制中心根据这些报告来进行相应的决策。
4. 内总线：可以保证数据的传输安全，拥有很高的吞吐量，更好控制。

有些既需要实时互动，又需要多人观看，所以就有了实时互动直播和泛娱乐化直播的融合：

由于实时直播互动架构采用的是udp协议，CDN网络采用的是rtmp协议，为了可以互相转换，在两个架构中间增加一个**媒体转换服务器**，通过内总线将数据传输到媒体服务器中，进行数据协议转换。之后用户就可以直接从流媒体云服务器获取rtmp流，直接观看。

#### 4. CDN网络

CDN网络：是为了解决用户访问资源慢而出现的一个技术。

为什么资源访问慢：

* 链路过长，比如东北的用户想要访问的服务在海南，所以链路需要从东北到海南，才能获取到数据。
* 运营商自己利益：人为造成的限制。

CDN构成：

* 边缘节点：用户从边缘节点获取数据；
* 二级节点：主干网节点，主要用于缓存，减轻源站压力；
* 源站：CP（内容提供方）将内容放到源站；

用户访问资源过程：

　　用户通过DNS或者http先找到就近边缘节点，如果数据没有，直接到主干节点，看资源是在哪提供的服务，如果是在联通上，就会到联通的主干节点上访问，如果数据没有，就会到联通的源节点上，如果找到数据，再通过原节点返回。如果是第一个用户，就会走很长时间，如果其他用户也访问此数据，那么数据就会缓存到边缘节点上，可以直接从边缘节点上直接访问，加快速度。

为了解决运营商之间的限制，一般会在两个运营商分别建设一个主干节点，通过光纤对接，将他们连接在一起。

传统的cdn网络会追求一些热点，将一些热点放到缓存里

对于直播的cnd网络，一般会通过源直接push到主干节点，让数据都存在，与推拉是相结合的。

#### 5. 常用工具

* ffmpeg

  音视频格式转换（mp4转换为flv）、音视频抽取（eg：小咖秀，合并音频和自录视频）、视频打水印、去水印、倍速播放、音视频剪辑。

* ffplay

  播放器，命令行的，基于ffmpeg的二次开发。

* flashplayer

  播放rtmp流的，可以对rtmp协议做分析

* webRTC

***

### 第二章 音频入门

#### 1. 声音三要素

* 音调：就是音频。男生 > 女生  > 儿童
* 音量：振动的幅度
* 音色：它与材质有很大的关系，本质是谐波。

音调对应正弦波的频率；音量对应正弦波的高度；音色越接近正弦波，声音一般越好听，畸形或产生噪波。

人类听觉范围是20HZ到20kHZ，音频压缩，就可以采集到很多数据，砍掉人类听不见的，减少数据存储量。

#### 2. 音频的量化与编码

* 量化基本概念

  * 采样大小：一般采样用多少bit存放。常用是16bit。

  * 采样率：采样频率8k、16k、32k、44.1k、48k

    AAC一般采用44.1k的采样率。44.1k代表1秒钟在模拟信号上采集44100次。

    假设我们对20HZ ~ 20kHZ进行采样，是用44.1k的采样率：

    * 20HZ代表1秒钟有20个正弦波，那么一个正弦波的采样次数是2000次。
    * 20kHZ代表1秒钟有20000个正弦波，那么一个正弦波采样次数是2次。

  * 声道数：单声道、双声道、多声道。每一个喇叭代表一个省道

* 码率计算

  码率 = 采样率 × 采样大小 × 声道数

  eg：采样率为44.1kHZ，采样大小为16bit，双声道的PCM编码的WAV文件，它的码率为：

  ```java
  44.1k × 16 × 2 = 1411.2kb/s = 1411.2kb/s / 8bit = 176.4KB/s
  ```

#### 3. 音频压缩技术讲解

一般有两种方法：

* 消除冗余数据

  有损压缩，将我们识别不到的部分删除掉，删除后无法恢复。

* 哈夫曼无损编码

音频冗余数据

* 压缩的主要方法是去除采集到的音频冗余信息，所谓冗余信息包括人耳听觉范围外的音频信号以及被掩蔽掉的音频信号。
* 信号的掩蔽分为频域掩蔽和时域掩蔽。

频域掩蔽：低于20Hz，高于20kHz的声音屏蔽掉；在这之间的，分贝比较低的屏蔽掉；在一个大的声音周边比较小的声音也会被屏蔽掉。

时域掩蔽：持续的时间内有一个比较高的声音，一个比较低的声音，那么声音比较低的那个会被屏蔽；

音频的编码过程：

#### 4. 音频编解码器选型

常见的音频编码器包括：OPUS、AAC、Vorbis、Speex、iLBC、AMR、G.711等。

* 实时互动系统可以用opus

* 泛娱乐化直播一般使用AAC，opus一般不支持，推广上有些困难
* speex：回音消除，降噪模块等可实现
* G.711:有些会与固话相联系，固话用的就是G.711，或者G.722
* 网上评测结果：OPUS > AAC > Vorbis

#### 5. AAC讲解

为什么介绍AAC：

* 应用范围广，直播系统90%以上使用

* 传输协议，像cdn，支持rtmp，但是不支持opus

* AAC的音频编解码可以保持高保真

AAC出现的原因：

* AAC目的是取代mp3格式。
* MPEG-4标准出现后，AAC加入了SBR技术和PS技术
* 目前常用的规格有：AAC LC、AAC HE V1、AAC HE V2

AAC规格描述：

* AAC LC：（Low Complexity） 低复杂度，码流128k
* AAC HE V1 = AAC LC + SBR（Spectral Band Relication）
* AAC HE V2 = AAC HE V1 + PS（Parametric Stereo）
* SBR：降低低频的采样率，提高对高频的采样率，降低大小，提高音质
* PS：把立体声保存，一个声道完整保存，另一个声道只存差异部分
* AAC HE：码率64k；AAC HE V2：码率32k

AAC格式：

* ADIF（Audio Data Interchange Format）

  这种格式只能从头开始解码，常用于磁盘文件中

* ADTS（Audio Data Transport Stream）

  这种格式每一帧都有一个同步字，可以在音频流的任何位置开始解码，它类似于数据流格式。

* rtmp和flv中都有adts的格式。

***

### 第三章 视频入门

#### 1. 视频基础知识

一般视频文件30帧左右，要求比较高的可以达到60帧。

H264基本概念：

* I 帧：关键帧，采用帧内压缩技术
* P 帧：向前参考帧，压缩时之参考前一个帧，属于帧间压缩技术
* B 帧：双向参考帧，压缩时既参考前一帧也参考后一帧，帧间压缩技术。

假设一个摄像头对着我们，我们在一秒钟内没有太大幅度的变化，对于这一组帧，变化幅度不大，为了压缩数据边下，我们会将第一帧完成的保存下来，这就是I 帧。I 帧之后的每一个帧，都是向前依赖的，并且之存取它与前一帧的差异，这样就可以大大减少存储数据，这就是P帧；B帧是前后参考帧，即参考前一帧又参考后一帧，这样压缩效率更高，这也是B帧的好处。B帧不适合实时互动，因为需要等待后面的参考帧，所以实时互动一般不选择B帧。

GOF：一个I 帧和另一个I 帧之间的一组帧。

SPS和PPS：

* SPS:（Sequence Parameter Set），序列参数集，存放帧数、参考帧数数目、解码图像尺寸、帧场编码模式选择标识等。
* PPS：(Picture Parameter Set)，图像参数集，存放熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系数调整标识等。
* 在一组帧之前，首先会收到sps和pps，如果没有这两个参数是没法解码的。sps和pps划分为I 帧。

视频花屏 / 卡顿原因：

* 如果GOP分组中的P帧丢失会导致解码端的图像发生错误
* 为了避免花屏问题的发生，一般如果发现P帧或者I帧丢失，就不显示本GOP内的所有帧，知道下一个I 帧来后重新刷新图像。
* 花屏是因为丢了数据，卡顿是因为怕花屏主动丢了一组数据。

视频有哪些视频编码器？

* x264 / x265

  x265压缩比更高，占用的CPU也就更高，在直播系统中不太适合。点播系统可以尝试使用

* openH264

  支持SVC视频技术。SVC是分层传输技术，一帧数据分为小中大三部分，网络差只发小的数据，稍微好点中间也发出去，网络非常好就大的也发出去，每加一层就清晰一些。很多移动端不支持SVC技术，所以只能使用软编，使用软编对CPU消耗更大。

* google系列的vp8 / vp9。vp8对应x264，vp9对应x265

#### 2. H264宏块的划分和与帧分组

H264压缩技术：

* 帧内预测压缩，解决的是空域数据冗余问题。
* 帧间预测压缩，解决的是时域数据冗余问题。
* 正数离散余弦变换（DCT），将空间上的相关性变为频域上无关的数据然后进行量化。
* CABAC压缩（无损压缩）

H264宏块划分、子块划分、帧分组。

#### 3. 视频压缩技术详解



#### 4. H264结构与码流

一个视频序列的每一帧由多个片组成，每个片又由多个宏块组成，每个宏块又会分成多个子块。	

H264编码分层：

* NAL层：Network Abstraction Layer，视频数据网络抽象层。

  H264最终还是要在网络上进行传输，但是每个网络包最大为1500字节（MTU：最大传输单元），一个H264的帧往往大于1500字节，所以需要进行拆包，将一个帧拆成多个包，这个过程通过NAL进行处理。

* VCL层：Video Coding Layer，视频数据编码层。

  对视频原始数据进行压缩。

码流基本概念：

* SODB：String Of Data Bits，原始数据比特流，长度不一定是8的倍数，它由VCL层产生。

  计算机中处理的字节都是以8的整数倍处理，所以处理比较麻烦

* RBSP：Raw Byte Sequence Payload，SODB + trailing bits，算法是在SODB最后以为补1，不按字节对齐则补0。补1是为了知道流的结束位，补0是为了凑够8的整数倍。

* EBSP：Encapsulate Byte Sequence Payload，需要两个连续的0x00就增加一个0x03

  在每一个帧的开头加一个起始位，起始位一般是16进制的0x0000或者0x0001再或者0x000001，但是0x0000和0x000001会冲突，所以遇到两个连续的00，就增加一个03，用来避免冲突：

  ```
  0x000001 -> 0x00000301
  ```

* NALU：NAL Header(1B) + EBSP

NAL Unit：一个NAL头加上一个H264视频帧的切片，切片还可以分为切片头和切片数据。数据传输的时候，就将一个H264拆开多个片，每个片加上一个NAL头进行传输。

<img src="https://static.oschina.net/uploads/space/2018/0507/191642_WVBn_3018050.png" alt="img " />

#### 5. NAL单元详解

NAL Header：

```
+---------------+
|0|1|2|3|4|5|6|7|
+---------------+
|F|NRI|   Type  |
+---------------+
```

* F：forbidden_zero_bit，在H264规范中规定这一位必须是0。
* NRI：指示重要性，00不重要，11最重要，暂无用。
* Type：这个NAL单元的类型，常用的类型如下：
  * 5：IDR图像的片
  * 7：序列参数集
  * 8：图像参数集
  * 28：FU-A，分片的单元
  * 29：FU-A，分片的单元

NAL类型介绍：

* 单一类型：一个RTP只包含一个NALU
* 组合类型：一个RTP包含多个NALU，类型是24~27
* 分片类型：一个NALU单元分成多个RTP包，类型是28~29

FU Header：

```
+---------------+
|0|1|2|3|4|5|6|7|
+---------------+
|S|E|R|   Type  |
+---------------+
```

* S：start bit，用于指示分片的开始
* E：end bit，用于指示分片的结束
* R：未使用，设置为0。
* Type：执行NAL分片类型。

#### 6. YUN讲解	

YUV：也称YCBCr，是电视系统采用的一种颜色编码方法。

* Y表示明亮度，也就是灰阶值，它是基础信号。
* U和V表示的则是色度，UV的作用是描述影响色彩及饱和度，它们用于指定像素的颜色。

YUV常见格式：

* YUV 4:2:0

* YUV 4:2:2

  4个Y对应两个U对应两个V

* YUV 4:4:4

比RGB是8:8:8节省空间。

YUV 4:2:0

* YUV 4:2:0并不是意味着只有Y和U两个分量，而没有V分量。它实际指的是对每行扫描线来说，只有一个色度分量，它以2：1的抽样率存储。
* 相邻的扫描行存储不同的色度分量，也就是说，如果一行是4:2:0的话，下一行就是4:0:2，再下一行就是4:2:0。以此类推。

YUV的存储格式

* 

***

### 参考文章

* [慕课网视频地址](https://www.imooc.com/video/16795)
* [移动端音频视频入门](https://my.oschina.net/u/3018050/blog/1788972)

